{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating random text data for Pandas\n",
    "\n",
    "## Creating random length sentences of random length words using random characters\n",
    "\n",
    "### Background\n",
    "\n",
    "A friend recently asked about transforming data in a Pandas data frame and provided an example of what the target column contains currently (prior to the transformation).\n",
    "\n",
    "After I and a few others made some suggestions, there was also some discussion on performance of the different method of applying the desired transformation.\n",
    "\n",
    "So that we could properly test, I decided to make some test data, and this in itself provided a challenge.\n",
    "\n",
    "Briefly, the column of data contained JSON data with:\n",
    "\n",
    "1. A single-element array\n",
    "2. Containing an Object (actually, more like a set in Python)\n",
    "3. Containing one or more words separated by commas\n",
    "\n",
    "An example (similar to that provided by my friend):\n",
    "\n",
    "`[{These, words, are, contained, by, an, Object, or, set, which, itself, is, contained, by, an, array}]`  \n",
    "`[{The, next, set, of, words, is, like, this}]`\n",
    "\n",
    "As yet, it is not clear whether the data contained the individual words wrapped by double quotes or not, and possibly the data was actually\n",
    "\n",
    "`[{\"These\", \"words\", \"are\", \"contained\", \"by\", \"an\", \"Object\", \"or\", \"set\", \"which\", \"itself\", \"is\", \"contained\", \"by\", \"an\", \"array\"}]`  \n",
    "`[{\"The\", \"next\", \"set\", \"of\", \"words\", \"is\", \"like\", \"this\"}]`\n",
    "\n",
    "Having this example, the challenge is to create a lot of data with a random number of words in each \"sentence\", where each word contains a random number of letters.\n",
    "\n",
    "Just the notion of the word \"random\" implies using `numpy.random` however, none of the methods available in that class can help us directly in our task. For that, we will also need to make use of `itertools` and a not so well known (well, at east to me) method of splitting an array by unequal length intervals using `numpy.split`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from numpy import random as rand\n",
    "import numpy as np\n",
    "import itertools as itools\n",
    "#import unicodedata\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_RECORDS = 1000000\n",
    "MAX_WORDS_PER_SENTENCE = 20\n",
    "MAX_CHARS_PER_WORD = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a \"template\" (of sorts) for our records.\n",
    "# This template will tell us how many words randomly will be in each record.\n",
    "sentence_templates = rand.randint(low=1, high=MAX_WORDS_PER_SENTENCE+1, size=NUM_RECORDS)\n",
    "\n",
    "# Create another \"template\" which will be a random number of characters for each word.\n",
    "# We know the count of all words across all records by using the sum() method of the numpy array sentence_templates\n",
    "word_templates = rand.randint(low=1, high=MAX_CHARS_PER_WORD+1, size=sentence_templates.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10513416"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's just see how many words we will have in total\n",
    "len(word_templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now use itertools to creating a running count of words\n",
    "# For example, if the first 5 sentences (records) had word counts of 7, 3, 4, 8, 2\n",
    "# itertools would return 7, 10, 14, 22, 24\n",
    "# This running count is needed in the numpy.split method later\n",
    "words_iter = list(itools.accumulate(word_templates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's create an array of random characters from A-Z,\n",
    "# having length = the last element of our word iterator.\n",
    "# Similar to the example of 5 above where 24 would be returned as the size.\n",
    "asc_codes = rand.randint(low=65, high=91, size=words_iter[-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([72, 84, 65, 90, 74, 88, 71, 78, 83, 75, 74, 66, 89, 71, 72, 71, 73,\n",
       "       66, 76, 72, 90, 80, 87, 79, 82, 74, 80, 77, 78, 79, 75, 79, 85, 80,\n",
       "       73, 86, 73, 75, 68, 89, 88, 68, 81, 68, 82, 84, 81, 70, 87, 80])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a peek at the ascii codes that were generated\n",
    "asc_codes[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['H',\n",
       " 'T',\n",
       " 'A',\n",
       " 'Z',\n",
       " 'J',\n",
       " 'X',\n",
       " 'G',\n",
       " 'N',\n",
       " 'S',\n",
       " 'K',\n",
       " 'J',\n",
       " 'B',\n",
       " 'Y',\n",
       " 'G',\n",
       " 'H',\n",
       " 'G',\n",
       " 'I',\n",
       " 'B',\n",
       " 'L',\n",
       " 'H']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#f = lambda x: chr(x)\n",
    "#chars = f(asc_codes.tolist())\n",
    "# The above did not work, so we will use the following instead\n",
    "# to convert the ascii codes to characters\n",
    "chars = [chr(x) for x in asc_codes.tolist()]\n",
    "chars[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HTAZJ',\n",
       " 'XGNSKJBYGH',\n",
       " 'GIBLHZPWOR',\n",
       " 'JPM',\n",
       " 'NO',\n",
       " 'KOUPIVIKD',\n",
       " 'YXDQDRT',\n",
       " 'QFWPDAMTE',\n",
       " 'TFNTIXY',\n",
       " 'LJKLH',\n",
       " 'H',\n",
       " 'JXOIGIXY',\n",
       " 'LQRSU',\n",
       " 'DTFTOSYQI',\n",
       " 'B',\n",
       " 'OWYTLDQ',\n",
       " 'JFVDHYOISG',\n",
       " 'SBDZWKS',\n",
       " 'LM',\n",
       " 'GZZIM']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Taking the long list of characters that was generated by our conversion from ascii to char\n",
    "# we use the numpy.split() method in which the 2nd parameter tells us at which index locations\n",
    "# to split the array (this is why ithertools was handy)\n",
    "words = [''.join(x) for x in np.split(chars, words_iter)]\n",
    "\n",
    "# Let's see the first 20 words we created\n",
    "words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20,\n",
       " 22,\n",
       " 34,\n",
       " 43,\n",
       " 44,\n",
       " 63,\n",
       " 72,\n",
       " 85,\n",
       " 91,\n",
       " 94,\n",
       " 105,\n",
       " 120,\n",
       " 133,\n",
       " 145,\n",
       " 150,\n",
       " 163,\n",
       " 180,\n",
       " 188,\n",
       " 196,\n",
       " 211]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# At this point, we have a list of words but we still need to split those words up into sentences (or records)\n",
    "# Once again, we use itertools to accumulate the count so that we can pass this to the numpy.split() method\n",
    "sentences_iter = list(itools.accumulate(sentence_templates))\n",
    "\n",
    "sentences_iter[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[{HTAZJ, XGNSKJBYGH, GIBLHZPWOR, JPM, NO, KOUPIVIKD, YXDQDRT, QFWPDAMTE, TFNTIXY, LJKLH, H, JXOIGIXY, LQRSU, DTFTOSYQI, B, OWYTLDQ, JFVDHYOISG, SBDZWKS, LM, GZZIM}]',\n",
       " '[{ILJK, FJPUS}]',\n",
       " '[{EN, YMIDL, B, U, KLWEWCXXIY, TNZNSKP, NXJFRZSWTC, TWMSL, I, PXR, UYMDVC, QXVPYXRPH}]',\n",
       " '[{NCW, NFVEDSXKZD, ERJOVTZTCM, MMBVQD, MEGWZXK, IXAKQ, IL, XEAMSA, DH}]',\n",
       " '[{VIZBOP}]',\n",
       " '[{NVPS, RFQDMTFUHS, EF, QHVBTNWFM, RI, MWFIECKRJ, WPJXNK, PQVKBFLL, LS, IDDVCQTXWW, MBZMV, VEAQG, TID, JLSWEOZHQ, BEJFOZEO, ZB, KUISMIVAS, OQOKS, DUKL}]',\n",
       " '[{POSUXULJY, MFZOATJ, POWKXUONC, NR, MRPONJVE, KWDLJVQ, MIKOG, WCSZDGEC, UZYOXAVRS}]',\n",
       " '[{ZRRKN, GVHJNYJ, PQCQFU, DPQQ, AW, QVZXJTT, RISZUFAU, LGTMJ, ZI, DCATPH, OB, KETD, EOJMICQ}]',\n",
       " '[{VPNAS, HTKNOXUDKT, D, EXHMXPF, TYHTGKPET, FBVFSSW}]',\n",
       " '[{HAZYHOQ, FFLYRIKRKN, EBUMR}]']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we apply the numpy.split method, but this time to a list of words rather than a list of characters\n",
    "\n",
    "# For the source data, if we want the individual words to be wrapped in quotes, use:\n",
    "#sentences = ['[{\"' + str('\", \"'. join(x)) + '\"}]' for x in np.split(words, sentences_iter)]\n",
    "# For the source data, if we want no quotes around the individual words, use:\n",
    "sentences = ['[{' + str(', '. join(x)) + '}]' for x in np.split(words, sentences_iter)]\n",
    "sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the data - we typically have an empty \"sentence\" as the last one\n",
    "if sentences[-1:] == ['[{}]']:\n",
    "    sentences = sentences[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JSON_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{HTAZJ, XGNSKJBYGH, GIBLHZPWOR, JPM, NO, KOUP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{ILJK, FJPUS}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{EN, YMIDL, B, U, KLWEWCXXIY, TNZNSKP, NXJFRZ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[{NCW, NFVEDSXKZD, ERJOVTZTCM, MMBVQD, MEGWZXK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[{VIZBOP}]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          JSON_words\n",
       "0  [{HTAZJ, XGNSKJBYGH, GIBLHZPWOR, JPM, NO, KOUP...\n",
       "1                                    [{ILJK, FJPUS}]\n",
       "2  [{EN, YMIDL, B, U, KLWEWCXXIY, TNZNSKP, NXJFRZ...\n",
       "3  [{NCW, NFVEDSXKZD, ERJOVTZTCM, MMBVQD, MEGWZXK...\n",
       "4                                         [{VIZBOP}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that we have a ist of \"sentences\", we can push them into a Pandas data frame\n",
    "df_sentences = pd.DataFrame(sentences, columns=[\"JSON_words\"])\n",
    "df_sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now, we can save the data frame so that\n",
    "# we can repeatedly test against the same data using different methods\n",
    "df_sentences.to_csv(\"create_random_words_test_data.csv\", sep=\"|\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JSON_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{HTAZJ, XGNSKJBYGH, GIBLHZPWOR, JPM, NO, KOUP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{ILJK, FJPUS}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{EN, YMIDL, B, U, KLWEWCXXIY, TNZNSKP, NXJFRZ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[{NCW, NFVEDSXKZD, ERJOVTZTCM, MMBVQD, MEGWZXK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[{VIZBOP}]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          JSON_words\n",
       "0  [{HTAZJ, XGNSKJBYGH, GIBLHZPWOR, JPM, NO, KOUP...\n",
       "1                                    [{ILJK, FJPUS}]\n",
       "2  [{EN, YMIDL, B, U, KLWEWCXXIY, TNZNSKP, NXJFRZ...\n",
       "3  [{NCW, NFVEDSXKZD, ERJOVTZTCM, MMBVQD, MEGWZXK...\n",
       "4                                         [{VIZBOP}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data back from the file we saved, and re-display the head\n",
    "df_map = pd.read_csv(\"create_random_words_test_data.csv\", sep=\"|\")\n",
    "df_map.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply one of the transformations as an example\n",
    "\n",
    "# If the source data does not contain words wrapped with double quotes, use:\n",
    "df_map[\"JSON_words\"] = df_map[\"JSON_words\"].map(lambda x: ['\"' + y + '\"' for y in re.split(\"[, ]+\", x[2:-2])])\n",
    "# If the source data already contains words wrapped with double quotes, use:\n",
    "#df_map[\"JSON_words\"] = [re.sub(\"[{}]\", \"\", str(x)) for x in df_map[\"JSON_words\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JSON_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[\"HTAZJ\", \"XGNSKJBYGH\", \"GIBLHZPWOR\", \"JPM\", \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[\"ILJK\", \"FJPUS\"]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[\"EN\", \"YMIDL\", \"B\", \"U\", \"KLWEWCXXIY\", \"TNZNS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[\"NCW\", \"NFVEDSXKZD\", \"ERJOVTZTCM\", \"MMBVQD\", ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[\"VIZBOP\"]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          JSON_words\n",
       "0  [\"HTAZJ\", \"XGNSKJBYGH\", \"GIBLHZPWOR\", \"JPM\", \"...\n",
       "1                                  [\"ILJK\", \"FJPUS\"]\n",
       "2  [\"EN\", \"YMIDL\", \"B\", \"U\", \"KLWEWCXXIY\", \"TNZNS...\n",
       "3  [\"NCW\", \"NFVEDSXKZD\", \"ERJOVTZTCM\", \"MMBVQD\", ...\n",
       "4                                         [\"VIZBOP\"]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_map.head()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f8b40d688a12481f01eadf7380c47edd8a49484a47dba3db091451640e880c68"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
