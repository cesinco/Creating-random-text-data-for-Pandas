{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating random text data for Pandas\n",
    "\n",
    "## Creating random length sentences of random length words using random characters\n",
    "\n",
    "### Background\n",
    "\n",
    "A friend recently asked about transforming data in a Pandas data frame and provided an example of what the target column contains currently (prior to the transformation).\n",
    "\n",
    "After I and a few others made some suggestions, there was also some discussion on performance of the different method of applying the desired transformation.\n",
    "\n",
    "So that we could properly test, I decided to make some test data, and this in itself provided a challenge.\n",
    "\n",
    "Briefly, the column of data contained JSON data with:\n",
    "\n",
    "1. A single-element array\n",
    "2. Containing an Object (actually, more like a set in Python)\n",
    "3. Containing one or more words separated by commas\n",
    "\n",
    "An example (similar to that provided by my friend):\n",
    "\n",
    "`[{These, words, are, contained, by, an, Object, or, set, which, itself, is, contained, by, an, array}]`  \n",
    "`[{The, next, set, of, words, is, like, this}]`\n",
    "\n",
    "As yet, it is not clear whether the data contained the individual words wrapped by double quotes or not, and possibly the data was actually\n",
    "\n",
    "`[{\"These\", \"words\", \"are\", \"contained\", \"by\", \"an\", \"Object\", \"or\", \"set\", \"which\", \"itself\", \"is\", \"contained\", \"by\", \"an\", \"array\"}]`  \n",
    "`[{\"The\", \"next\", \"set\", \"of\", \"words\", \"is\", \"like\", \"this\"}]`\n",
    "\n",
    "Having this example, the challenge is to create a lot of data with a random number of words in each \"sentence\", where each word contains a random number of letters.\n",
    "\n",
    "Just the notion of the word \"random\" implies using `numpy.random` however, none of the methods available in that class can help us directly in our task. For that, we will also need to make use of `itertools` and a not so well known (well, at east to me) method of splitting an array by unequal length intervals using `numpy.split`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from numpy import random as rand\n",
    "import numpy as np\n",
    "import itertools as itools\n",
    "#import unicodedata\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_RECORDS = 1000 # Make it 1000000 if you want (smaller set only to keep the csv and possibly the ipynb file smaller)\n",
    "MAX_WORDS_PER_SENTENCE = 20\n",
    "MAX_CHARS_PER_WORD = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a \"template\" (of sorts) for our records.\n",
    "# This template will tell us how many words randomly will be in each record.\n",
    "sentence_templates = rand.randint(low=1, high=MAX_WORDS_PER_SENTENCE+1, size=NUM_RECORDS)\n",
    "\n",
    "# Create another \"template\" which will be a random number of characters for each word.\n",
    "# We know the count of all words across all records by using the sum() method of the numpy array sentence_templates\n",
    "word_templates = rand.randint(low=1, high=MAX_CHARS_PER_WORD+1, size=sentence_templates.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10293"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's just see how many words we will have in total\n",
    "len(word_templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now use itertools to creating a running count of words\n",
    "# For example, if the first 5 sentences (records) had word counts of 7, 3, 4, 8, 2\n",
    "# itertools would return 7, 10, 14, 22, 24\n",
    "# This running count is needed in the numpy.split method later\n",
    "words_iter = list(itools.accumulate(word_templates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's create an array of random characters from A-Z,\n",
    "# having length = the last element of our word iterator.\n",
    "# Similar to the example of 5 above where 24 would be returned as the size.\n",
    "asc_codes = rand.randint(low=65, high=91, size=words_iter[-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([72, 79, 86, 83, 66, 73, 90, 71, 66, 78, 67, 82, 78, 78, 68, 89, 86,\n",
       "       69, 73, 86, 86, 69, 89, 74, 81, 83, 78, 66, 76, 68, 77, 86, 73, 83,\n",
       "       77, 90, 76, 84, 71, 84, 78, 78, 79, 71, 76, 72, 80, 73, 89, 80])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a peek at the ascii codes that were generated\n",
    "asc_codes[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['H',\n",
       " 'O',\n",
       " 'V',\n",
       " 'S',\n",
       " 'B',\n",
       " 'I',\n",
       " 'Z',\n",
       " 'G',\n",
       " 'B',\n",
       " 'N',\n",
       " 'C',\n",
       " 'R',\n",
       " 'N',\n",
       " 'N',\n",
       " 'D',\n",
       " 'Y',\n",
       " 'V',\n",
       " 'E',\n",
       " 'I',\n",
       " 'V']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#f = lambda x: chr(x)\n",
    "#chars = f(asc_codes.tolist())\n",
    "# The above did not work, so we will use the following instead\n",
    "# to convert the ascii codes to characters\n",
    "chars = [chr(x) for x in asc_codes.tolist()]\n",
    "chars[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HO',\n",
       " 'VSBIZ',\n",
       " 'GBN',\n",
       " 'CR',\n",
       " 'NNDYVE',\n",
       " 'IVVEYJQSNB',\n",
       " 'L',\n",
       " 'DM',\n",
       " 'VISMZLT',\n",
       " 'GT',\n",
       " 'NNOGL',\n",
       " 'HPIYP',\n",
       " 'YNALADVSAQ',\n",
       " 'K',\n",
       " 'JYKEGFHK',\n",
       " 'QVEUMAB',\n",
       " 'UWHKT',\n",
       " 'XAQWXCJLXF',\n",
       " 'BS',\n",
       " 'E']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Taking the long list of characters that was generated by our conversion from ascii to char\n",
    "# we use the numpy.split() method in which the 2nd parameter tells us at which index locations\n",
    "# to split the array (this is why ithertools was handy)\n",
    "words = [''.join(x) for x in np.split(chars, words_iter[:-1])]\n",
    "\n",
    "# Let's see the first 20 words we created\n",
    "words[:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19,\n",
       " 29,\n",
       " 42,\n",
       " 46,\n",
       " 56,\n",
       " 75,\n",
       " 77,\n",
       " 87,\n",
       " 104,\n",
       " 115,\n",
       " 130,\n",
       " 140,\n",
       " 152,\n",
       " 166,\n",
       " 173,\n",
       " 190,\n",
       " 209,\n",
       " 216,\n",
       " 217,\n",
       " 218]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# At this point, we have a list of words but we still need to split those words up into sentences (or records)\n",
    "# Once again, we use itertools to accumulate the count so that we can pass this to the numpy.split() method\n",
    "sentences_iter = list(itools.accumulate(sentence_templates))\n",
    "\n",
    "sentences_iter[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[{HO, VSBIZ, GBN, CR, NNDYVE, IVVEYJQSNB, L, DM, VISMZLT, GT, NNOGL, HPIYP, YNALADVSAQ, K, JYKEGFHK, QVEUMAB, UWHKT, XAQWXCJLXF, BS}]',\n",
       " '[{E, ZLGHBQVMLP, KMWTCT, RRQVHEHGLO, Q, ISQOQFUHAF, CB, SPHJE, KIIPT, TBNYVSV}]',\n",
       " '[{VUNFJGD, OWY, X, EBCOAETC, BYZYGNAXOC, ZCOMTJVB, AT, IYXHI, MOXTG, RXVAHA, J, WFIOOCSO, RJXBYHRNP}]',\n",
       " '[{IZO, YOFZAGOCT, TXRQGW, MF}]',\n",
       " '[{DUVKRNYHT, F, GWFIIUBYW, H, WKLEOSHCK, SKWPTPS, TPIMGLCUEI, VUACPAF, GVYHJVXZX, EER}]',\n",
       " '[{NA, QPE, GO, KSPHATCKZW, FFNHHZ, SZBB, MMATRGIEUR, HLHEJWQ, XLA, L, MFTRJ, CZD, NHITJGVZN, REHVC, VTVIR, I, INXP, NEPOZZDGC, MGDP}]',\n",
       " '[{RDRQ, UJGPJYNK}]',\n",
       " '[{NKXXAV, QJG, KDHNULPZM, JFUENMMDIH, BHPIRIZF, YOQYBJ, FIHZFQ, PIF, L, FHZCVUX}]',\n",
       " '[{NBR, HTKKYEZUD, UBMKBH, GITBIPAW, DIOFWYO, PWS, TUEX, LWHC, YFMSXQQVTM, OVFYAEZB, HGNS, FFCYHROQA, TVLB, UGACUBTEVU, RTYYUHVZNZ, BSBNU, LRQKNQKEQD}]',\n",
       " '[{WXBBF, AXXJR, IFBHNSHCR, VKQGZBRT, O, Y, SDHLYCXL, AVLVA, TCGLKPT, XXWGJE, R}]']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we apply the numpy.split method, but this time to a list of words rather than a list of characters\n",
    "\n",
    "# For the source data, if we want the individual words to be wrapped in quotes, use:\n",
    "#sentences = ['[{\"' + str('\", \"'. join(x)) + '\"}]' for x in np.split(words, sentences_iter[:-1])]\n",
    "# For the source data, if we want no quotes around the individual words (as originally presented by my friend), use:\n",
    "sentences = ['[{' + str(', '. join(x)) + '}]' for x in np.split(words, sentences_iter[:-1])]\n",
    "sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JSON_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{HO, VSBIZ, GBN, CR, NNDYVE, IVVEYJQSNB, L, D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{E, ZLGHBQVMLP, KMWTCT, RRQVHEHGLO, Q, ISQOQF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{VUNFJGD, OWY, X, EBCOAETC, BYZYGNAXOC, ZCOMT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[{IZO, YOFZAGOCT, TXRQGW, MF}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[{DUVKRNYHT, F, GWFIIUBYW, H, WKLEOSHCK, SKWPT...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          JSON_words\n",
       "0  [{HO, VSBIZ, GBN, CR, NNDYVE, IVVEYJQSNB, L, D...\n",
       "1  [{E, ZLGHBQVMLP, KMWTCT, RRQVHEHGLO, Q, ISQOQF...\n",
       "2  [{VUNFJGD, OWY, X, EBCOAETC, BYZYGNAXOC, ZCOMT...\n",
       "3                     [{IZO, YOFZAGOCT, TXRQGW, MF}]\n",
       "4  [{DUVKRNYHT, F, GWFIIUBYW, H, WKLEOSHCK, SKWPT..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that we have a list of \"sentences\", we can push them into a Pandas data frame\n",
    "df_sentences = pd.DataFrame(sentences, columns=[\"JSON_words\"])\n",
    "df_sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now, we can save the data frame so that\n",
    "# we can repeatedly test against the same data using different methods\n",
    "df_sentences.to_csv(\"create_random_words_test_data.csv\", sep=\"|\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JSON_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{HO, VSBIZ, GBN, CR, NNDYVE, IVVEYJQSNB, L, D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{E, ZLGHBQVMLP, KMWTCT, RRQVHEHGLO, Q, ISQOQF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{VUNFJGD, OWY, X, EBCOAETC, BYZYGNAXOC, ZCOMT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[{IZO, YOFZAGOCT, TXRQGW, MF}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[{DUVKRNYHT, F, GWFIIUBYW, H, WKLEOSHCK, SKWPT...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          JSON_words\n",
       "0  [{HO, VSBIZ, GBN, CR, NNDYVE, IVVEYJQSNB, L, D...\n",
       "1  [{E, ZLGHBQVMLP, KMWTCT, RRQVHEHGLO, Q, ISQOQF...\n",
       "2  [{VUNFJGD, OWY, X, EBCOAETC, BYZYGNAXOC, ZCOMT...\n",
       "3                     [{IZO, YOFZAGOCT, TXRQGW, MF}]\n",
       "4  [{DUVKRNYHT, F, GWFIIUBYW, H, WKLEOSHCK, SKWPT..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data back from the file we saved, and re-display the head\n",
    "df_map = pd.read_csv(\"create_random_words_test_data.csv\", sep=\"|\")\n",
    "df_map.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply one of the transformations as an example\n",
    "\n",
    "# If the source data does not contain words wrapped with double quotes, use:\n",
    "df_map[\"JSON_words\"] = df_map[\"JSON_words\"].map(lambda x: ['\"' + y + '\"' for y in re.split(\"[, ]+\", x[2:-2])])\n",
    "# If the source data already contains words wrapped with double quotes, use:\n",
    "#df_map[\"JSON_words\"] = [re.sub(\"[{}]\", \"\", str(x)) for x in df_map[\"JSON_words\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JSON_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[\"HO\", \"VSBIZ\", \"GBN\", \"CR\", \"NNDYVE\", \"IVVEYJ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[\"E\", \"ZLGHBQVMLP\", \"KMWTCT\", \"RRQVHEHGLO\", \"Q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[\"VUNFJGD\", \"OWY\", \"X\", \"EBCOAETC\", \"BYZYGNAXO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[\"IZO\", \"YOFZAGOCT\", \"TXRQGW\", \"MF\"]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[\"DUVKRNYHT\", \"F\", \"GWFIIUBYW\", \"H\", \"WKLEOSHC...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          JSON_words\n",
       "0  [\"HO\", \"VSBIZ\", \"GBN\", \"CR\", \"NNDYVE\", \"IVVEYJ...\n",
       "1  [\"E\", \"ZLGHBQVMLP\", \"KMWTCT\", \"RRQVHEHGLO\", \"Q...\n",
       "2  [\"VUNFJGD\", \"OWY\", \"X\", \"EBCOAETC\", \"BYZYGNAXO...\n",
       "3               [\"IZO\", \"YOFZAGOCT\", \"TXRQGW\", \"MF\"]\n",
       "4  [\"DUVKRNYHT\", \"F\", \"GWFIIUBYW\", \"H\", \"WKLEOSHC..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_map.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JSON_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>[\"JCN\", \"HLGXI\", \"BDXX\", \"A\", \"ZUFVLGWIT\", \"JV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>[\"XCFLD\", \"HTBVRRUJT\", \"UFPFMMNEYC\", \"XEZPDNZYB\"]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>[\"BCVZJH\"]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>[\"CRM\", \"MXZKPLG\", \"AFMM\", \"KOHO\", \"KWIPO\", \"L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>[\"DRHXVSLSV\", \"YOWKJBUJ\", \"DQJZNA\", \"T\", \"XDFX...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            JSON_words\n",
       "995  [\"JCN\", \"HLGXI\", \"BDXX\", \"A\", \"ZUFVLGWIT\", \"JV...\n",
       "996  [\"XCFLD\", \"HTBVRRUJT\", \"UFPFMMNEYC\", \"XEZPDNZYB\"]\n",
       "997                                         [\"BCVZJH\"]\n",
       "998  [\"CRM\", \"MXZKPLG\", \"AFMM\", \"KOHO\", \"KWIPO\", \"L...\n",
       "999  [\"DRHXVSLSV\", \"YOWKJBUJ\", \"DQJZNA\", \"T\", \"XDFX..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_map.tail()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f8b40d688a12481f01eadf7380c47edd8a49484a47dba3db091451640e880c68"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
